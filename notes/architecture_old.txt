Architektur: CNN_FIRST
1. CNN
- conv_1: in_channels=1,   out_channels=32,  kernel_size=8, stride=4, padding=1
- conv_2: in_channels=32,  out_channels=64,  kernel_size=4, stride=2
- conv_2: in_channels=64,  out_channels=128, kernel_size=3, stride=2
--> input_size:   (3,   1, 84, 72)
--> output_size:  (3, 128,  4,  3)

2. Encoder / LSTM:
- input_size:  		1536
- hidden_size: 		1536
- nr_layers:	  	   3
- bidirectional:   False
--> input_size:   (3, 1, 1536)
--> output_size:  (3, 1, 1536)

3. Attention:
- hidden_size:      1536
- alignment: 'location' --> a_t = softmax(W_a, h_t) --> W_a = nn.Linear(hidden_size, hidden_size, bias=False)
- attention_mechanism: global --> softmax(alignment_score)
--> input_size:   (3, 1, 1536)
--> output_size:  (3, 1, 1536)

4. Decoder / LSTM:
- input_size:  		1536 x 2 = 3072  (concat)
- hidden_size: 		1536
- nr_layers:	  	   1
- bidirectional:   False
--> input_size:   (3, 1, 3072)
--> output_size:  (3, 1, 1536)

5. Fully Connected:
- input_size: 		1536
- output_size: 		nr_actions

map:
w   = width
h   = height
c   = channels
seq = sequence length
fi  = filters (channels)
b   = batch_size
fe  = features

Shapes:
#                     b  w    h    c                                    b  c  h   w
3 consecutive images (3, 210, 160, 3) --> transform --> 1 tensor shape (3, 1, 84, 72)

#                   b  c  h   w                                     b  fi   h  w
input tensor shape (3, 1, 84, 72) into CNN --> output tensor shape (3, 128, 4, 3)

#                        b  fi   h  w                                          fe = 128 x 4 x 3                                  seq  b   fe
CNN output tensor shape (3, 128, 4, 3) reshape to match encoder (LSTM) input -->    RESHAPE    --> tensor (reshaped) decoder_in (3,   1,  1536)

#			  	  seq  b   fe									  			seq  b   fe							   	   	 seq  b   fe 								seq  b   fe
decoder_in shape (3,   1,  1536) into encoder (LSTM) --> encoder_out shape (3,   1,  1536), encoder_hidden_states shape (3,   1,  1536), encoder_cell_states shape (3,   1,  1536)

# 											 	                 	   b   fe	 softmax(encoder_hidden_state, dim=1)				       b  fe
alignment score (location-based) from last encoder_hidden_state shape (1,  1536)  	--> alignment function --> 		alignment score shape (1, 1536)

#																		 	   b  fe																  	 b  fe
softmax alignment score to obtain attention weights --> alignment score shape (1, 1536) --> softmax(alignment_score, dim=1) --> attention_weights shape (1, 1536)

#						   		 b  fe						   		   fe
squeeze attention_weights shape (1, 1536) --> attention_weights shape (1536)

#								  fe							seq  b   fe										  seq  b   fe
multiply attention_weights shape (1536) with encoder_out shape (3,   1,  1536) to get context -->  context shape (3,   1,  1536)

#					  seq  b   fe						 	 seq  b   fe								concat(context, encoder_out, dim=2)						  seq  b   fe
concat context shape (3,   1,  1536) with encoder_out shape (3,   1,  1536) along last dimension dim=2 		--> obtain new decoder_in -->		decoder_in shape (3,   1,  3072)

# 					   seq  b   fe								   		 b   fe									   b   fe											     seq  b   fe
feed decoder_in shape (3,   1,  3072) , last encoder_hidden_state shape (1,  1536), last encoder_cell_state shape (1,  1536) into decoder (LSTM)  --> decoder_out shape (3,   1,  1536)

#							 b   fe
feed last decoder_out shape (1,  1536) in "QNet" to get q_value for each action  --> return action from highest corresponding q_value


============


Architektur: LSTM_FIRST

1. Encoder / LSTM:
- input_size:  		6048
- hidden_size: 		6048
- nr_layers:	  	   3
- bidirectional:   False
--> input_size:   (3, 1, 6048)
--> output_size:  (3, 1, 6048)

2. Attention:
- hidden_size:      6048
- alignment: 'location' --> a_t = softmax(W_a, h_t) --> W_a = nn.Linear(hidden_size, hidden_size, bias=False)
- attention_mechanism: global --> softmax(alignment_score)
--> input_size:   (3, 1, 6048)
--> output_size:  (3, 1, 6048)

3. Decoder / LSTM:
- input_size:  		6048 x 2 = 12096  (concat)
- hidden_size: 		6048
- nr_layers:	  	   1
- bidirectional:   False
--> input_size:   (3, 1, 12096)
--> output_size:  (3, 1, 6048)

4. CNN
- conv_1: in_channels=1,   out_channels=32,  kernel_size=8, stride=4, padding=1
- conv_2: in_channels=32,  out_channels=64,  kernel_size=4, stride=2
- conv_2: in_channels=64,  out_channels=128, kernel_size=3, stride=2
--> input_size:   (3,   1, 84, 72)
--> output_size:  (3, 128,  4,  3)

5. Fully Connected:
- input_size: 		1536
- output_size: 		nr_actions

map:
w   = width
h   = height
c   = channels
seq = sequence length
fi  = filters (channels)
b   = batch_size
fe  = features

Shapes:
#                     b  w    h    c                                    b  c  h   w
3 consecutive images (3, 210, 160, 3) --> transform --> 1 tensor shape (3, 1, 84, 72)

#					  b  c  h   w									fe = 1 x 84 x 72								  seq  b   fe
reshape tensor shape (3, 1, 84, 72) to match encoder (LSTM) input -->    RESHAPE    --> tensor (reshaped) decoder_in (3,   1,  6048)

#			  	  seq  b   fe									  seq  b   fe							   	   seq  b   fe 								  seq  b   fe
decoder_in shape (3,   1,  6048) into LSTM --> encoder_out shape (3,   1,  6048), encoder_hidden_states shape (3,   1,  6048), encoder_cell_states shape (3,   1,  6048)

# 											 	                 	   b   fe	 softmax(encoder_hidden_state, dim=1)				       b  fe
alignment score (location-based) from last encoder_hidden_state shape (1,  6048)  	--> alignment function --> 		alignment score shape (1, 6048)

#																		 	   b  fe																  	 b  fe
softmax alignment score to obtain attention weights --> alignment score shape (1, 6048) --> softmax(alignment_score, dim=1) --> attention_weights shape (1, 6048)

#						   		 b  fe						   		   fe
squeeze attention_weights shape (1, 6048) --> attention_weights shape (6048)

#								  fe								  		seq  b   fe										  seq  b   fe
multiply attention_weights shape (6048) with encoder_out encoder_out shape (3,   1,  6048) to get context -->  context shape (3,   1,  6048)

#					  seq  b   fe						 	 seq  b   fe								concat(context, encoder_out, dim=2)						  seq  b   fe
concat context shape (3,   1,  6048) with encoder_out shape (3,   1,  6048) along last dimension dim=2 		--> obtain new decoder_in -->		decoder_in shape (3,   1,  12096)

# 					   seq  b   fe								   		  b   fe									b   fe											    seq  b   fe
feed decoder_in shape (3,   1,  12096) , last encoder_hidden_state shape (1,  6048), last encoder_cell_state shape (1,  6048) into decoder LSTM  --> decoder_out shape (3,   1,  6048)

#						   seq  b   fe						  fe: 6048 = 84 x 72								 b  c  h   w
reshape decoder_out shape (3,   1,  6048) to match CNN input -->    RESHAPE    --> decoder_out (reshaped) shape (3, 1, 84, 72)

#										b  c  h   w								  b  fi   h  w
feed last decoder_out (reshaped) shape (1, 1, 84, 72) into CNN --> cnn_out shape (1, 128, 4, 3)

# 								 b  fi   h  w						   fe = 128 x 4 x 3			 		  b   fe
reshape / flatten cnn_out shape (1, 128, 4, 3) to match "QNet" input -->    RESHAPE    --> cnn_out shape (1,  1536)

#					b   fe
feed cnn_out shape (1,  1536) in "QNet" to get q_value for each action  --> return action from highest corresponding q_value

